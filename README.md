# deep-learning-challenge

OVERVIEW:
In the initial model, I used a two-layer architecture employed with 80 neurons in the first hidden layer, 30 neurons in the second hidden layer, and ReLU activation functions for both layers. The output layer, designed for binary classification, featured a single neuron with a sigmoid activation function. 

RESULTS:
Despite these choices, the model achieved only 73% accuracy, falling short of the 75% target.

OPTIMIZED:

1. The number of neurons was increased to enhance complexity.
   
2. Additional layers were added to provide more capacity.

3. Trying a different activation function (e.g., tanh for the second layer) and increasing epochs could improve the model's learning ability.

OPTIMIZED RESULTS:
None of the test emplimented resulted in 75% accuracy.

CONCLUSION:
By iteratively refining the model through strategic adjustments such as data augmentation, cleaning, algorithm exploration, and addressing biases, the aim is to systematically enhance accuracy and ensure optimal performance in the classification problem. Continuous fine-tuning remains integral to achieving the desired results.
